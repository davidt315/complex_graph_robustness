# Draft Review: Adding Heterogeneity to the Schelling Model
## For Ben and Mira. Review by David Tarazi

### 1. Question:  What is your understanding of the experiment the team is replicating?  What question does it answer?  How clear is the team's explanation?
- The team is replicating Shelling's model for segregation in cities as well as implementing a vision expansion model to look further into the neighborhood of each cell.
- The question the models are trying to answer from my understanding is trying to understand how far out into people's neighborhoods people care about alikeness and the expansion will look into whether or not people actually are looking for diversity in their neighborhoods.
- The expansion is a little unclear. It's hard to determine how the team will explore the want for diverse neighborhoods and what that experiment will look like. Many of the terms like heterogenity are loosely defined and it isn't super clear where the expanded vision model is coming from if it's from a paper or if that's an expansion itself. I think just some more context behind where things are coming from is needed. I also think a clear definition of experiments is needed. 


### 2. Methodology: Do you understand the methodology?  Does it make sense for the question?  Are there limitations you see that the team did not address?
- I understand the methodology for Shelling's model and the expanded size, but I don't quite understand the diverse neighborhood methodology. 
- I think the first method is a good showing for segregation and the expanded vision size is an interesting approach that will aid the question of segregation in neighborhoods, but hard to tell for the diverse neighborhoods.


### 3. Results: Do you understand what the results are (not yet considering their interpretation)?  If they are presented graphically, are the visualization#s effective?  Do all figures have labels on the axes and captions?
- The graph of results putting all the various sizes and segregation percentage is nearly impossible to read. The team needs to add more definition on what we're looking at and it feels like too much is going on. I think adding a title to that graph would be good and maybe only testing a few different percentages of segregation instead of like 10 would make that clearer. Additionally maybe just a graph that compares radius with one percentage of segregation first would make things clearer. Also looking at the 1 iteration vs 100 vs 1000 iterations of the model for different radiuses would be helpful to visualize how the results change.
- The figures don't all have lables on axes and a title. 


### 4. Interpretation: Does the draft report interpret the results as an answer to the motivating question?  Does the argument hold water?
- The results from the segregation model with a small window aren't super in depth, but is sufficient.
- There aren't much interpretations for how vision size actually changes the results and I don't know how to interpret the graph on my own so I'm not getting much out of that.


### 5. Replication: Are the results in the report consistent with the results from the original paper?  If so, how did the authors demonstrate that consistency?  Is it quantitative or qualitative?
- The results from the original paper aren't clear and there isn't a discussion of comparison between the article and the team's results. If anything there is a qualitative explanation there, but it would benefit from a quantitative comparison.


### 6. Extension: Does the report explain an extension to the original experiment clearly?  Can it answer an interesting question that the original experiment did not answer?
- The extension makes sense, but needs some more context and details, especially around the methodology. IT could definitely answer a question that the original experiment didn't which is super exciting!
- I think it's just a draft, but a lot more detail in the writing needs to be added for the extension to answer a question.
- It would be really cool to compare this to real cities and their segregation and see if there's a way to relate it to a real world context to support your results.


### 7. Progress: Is the team roughly where they should be at this point, with a replication that is substantially complete and an extension that is clearly defined and either complete or nearly so?
- I think the models are there with progress, but the writing is a little behind. Also the experiment definition for the extension is a little unclear and seems a bit behind, though I think most of the basis is there and the team isn't behind.


### 8. Presentation: Is the report written in clear, concise, correct language?  Is it consistent with the audience and goals of the report?  Does it violate any of the recommendations in my style guide (Links to an external site.)?
- The language is clear and concise, but needs more definitions of terms as the paper develops. Some of the language references Complexity Science which is the only issue there.
- The language is consistent with the audience and goals. 
- To me, it passes the Allen language check :)


### 9. Mechanics: Is the report in the right directory with the right file name?  Is it formatted professionally in Markdown?  Does it include a meaningful title and the full names of the authors?  Is the bibliography in an acceptable style? 
- Good formatting, but I think there's not a need to include "complexity science fall 2021" in the author's line.
- I think the papers used and where they align with your experiments isn't super clear and could use some in text references to the article authors.
- bibliography is good